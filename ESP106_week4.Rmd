---
title: "ESP106-Lab4"
author: "Fran M"
date: "January 29, 2023"
output: html_document
---

## Lab 4

In this lab we will look at daily tidy data downloaded from NOAA's Tides and Currents API (Application Programming Interface) for six cities around the US. I used the API to obtain six csv files containing data for tide gauges in each city. The tide gauges have numerical codes that correspond to the city as follows:

1. Boston: 8443970
2. New York: 8518750
3. Baltimore: 8574680
4. Charleston: 8665530
5. Miami: 8723214
6. Corpus Christi: 8775296

Before you start: add this file to your github repository, and commit your changes throughout the time you work on it. 

### Part 1 - Monday


# 1. Create a data frame containing data on the city name and tide gauge ID

```{r}
getwd
setwd("C:/Users/maddi/OneDrive/Desktop/R files/week four")
list.files(full.names=TRUE)
```

# 2a. Use a for-loop to read in the csv files and bind them together into a single data frame. Add a column to the data frame giving the name of the city the data is from.

```{r}
# USING LAPPY FUNCTION
files <- list.files(pattern = "\\.csv")
tables <- lapply(files, read.csv)
tables <- lapply(files, function(f) cbind(file=gsub(".csv$", "", f), read.csv(f)))
cities <- c("BOS", "NY", "BALT", "CHAR", "MIA", "CC")
# USING FOR-LOOP  
for (i in 1:length(cities)) {
  tables[[i]]$City <- cities[i]
}

combined <- do.call(rbind, tables)
str(combined)
head(combined)

# Manuel Way

tables[[1]]$City <- "BOS"
tables[[2]]$City <- "NY"
tables[[3]]$City <- "BALT"
tables[[4]]$City <- "CHAR"
tables[[5]]$City <- "MIA"
tables[[6]]$City <- "CC"

#Determing if there is NAs in table
is.na(combined)
sum(is.na(combined$HWI))
combined$HWI

```
### 2b. Take a look at your data frame - is this in a tidy format?

We are going to examine the question of whether these gauges show evidence of rising sea levels. One of the first things we have to deal with is the issue of dates.

Your data frame right now has one column with a year and one with the month. We are going to combine these into a single column, and use as.Date to formally use Date objects

# 3a. Create a new column named "Date" that has the first day of the month for that row in the format YYYY-MM-01 where YYYY is the data in the Year column and MM is the data in the Month column

# 3b. Use as.Date (INSTEAD USED lubridate::make_date) to convert your new date column to a date object in R

```{r}
combined$Date <- lubridate::make_date(year=combined$Year, month=combined$Month)
combined$Year <- NULL
combined$Month <- NULL
head(combined)
```
# 4. Make a plot showing data from all 6 gauges on the same plot. Use different colors to distinguish lines for the different cities. See the example plot uploaded to Canvas (Plot 1)


 * Plot the date on the x axis and MHHW (mean higher high water - i.e. the average daily high water level) on the y axis
Make sure to add proper axis labes and units (using +labs(x="",y=""))
 * Add a single best-fit line through the full data set using geom_smooth(method="lm") - note that by default ggplot will fit one best fit line for each city. To override this specify the aestetic mapping (aes()) again within the geom_smooth function and add the argument inherit.aes=FALSE
 
```{r, message=FALSE, warning=FALSE}
library(ggplot2) 
ggplot(combined, aes(x=Date, y=MHHW, group=City)) + 
  geom_line(aes(col=City)) +
  scale_color_manual(values = c("#4a6bc7", "#e43b75", "#e4c13b", "#0d400c", "#d5a0f9", "#7ed0c8")) + geom_smooth(method=lm, se=FALSE, col='black', linetype='dashed', linewidth=0.9, inherit.aes=FALSE, aes(x=Date, y=MHHW)) + theme_bw()
```

# 5. Now make a slightly different plot with the same x and y variables, but use facet_wrap() to make a subplot separately for each city. Add a best-fit line for each subplot. See the example plot uploaded to Canvas (Plot 2)

```{r, message=FALSE}
#Hint: you should only need minor modification of the code from question 4 to make this plot
library(ggplot2) 
ggplot(combined, aes(x=Date, y=MHHW, group=City)) + 
  geom_line(aes(col=City)) +
  scale_color_manual(values = c("#4a6bc7", "#e43b75", "#e4c13b", "#0d400c", "#d5a0f9", "#7ed0c8")) + facet_wrap(~ City) + geom_smooth(method=lm, se=FALSE, col='black', linetype='dashed', linewidth=0.4) + theme_bw()

```

### Part 2 - Wednesday

In this part of the lab we will identify some outliers, and practice running regressions

# 6. Make a box plot showing the distribution of the highest tides each month ("Highest" column in the NOAA data) . (Ideally practice using ggplot by using geom_boxplot() - put the city on the x axis and Highest on the y. But this can also be done in base R). See the example plot on Canvas (Plot 3)

```{r}
ggplot(combined, aes(x=Date, y=Highest, group=City)) + 
  geom_boxplot(aes(col=City)) +
  scale_color_manual(values = c("#4a6bc7", "#e43b75", "#e4c13b", "#0d400c", "#d5a0f9", "#7ed0c8")) + theme_bw()

```

Notice the very extreme value in New York City - a major outlier both within New York and compared to all the other cities

# 7a. Find the row in the data corresponding to this outlier observation 

```{r}
#Hint: The which.max() function might be useful here
which.max(combined$Highest)
#row = 141

```

# 7b. What month and year did this outlier event occur in? What meteorological event happened in New York in that month that probably caused this outlier event? (Feel free to use Google - I don't expect you to know this off hand)

```{r} 
combined$Highest[141,] 
# 2.739 is value
combined[141,]
# 2012-10 (Hurricane Sandy)
```

Finally, we will fit a linear model to estimate the rate of sea-level rise across these 6 cities.

# 8a. Fit a linear regression with the mean higher high water (MHHW) as the dependent variable and date (i.e. time) as the independent variable.

```{r}
#Hint: the forumla in your lm() function is of the form y~x where y here is MHHW and x is your date column
model <- lm(MHHW~Date,data=combined)
class(model)
print(model)
summary(model)$coefficients
summary(model)$r.squared
```

# 8b. Give the estimated coefficient of the date column. Is it statistically significant (i.e. has a p-value less than 0.05)?
```{r}
model <- lm(MHHW~Date,data=combined)
summary(m)$coefficients
# 2.732001e-05 AND -3.398793e-01 (statistically significant)

ggplot(combined, aes(x=Date, y=MHHW, group=City)) +
  geom_line(aes(col=City)) +
  scale_color_manual(values = c("#4a6bc7", "#e43b75", "#e4c13b", "#0d400c", "#d5a0f9", "#7ed0c8")) + geom_smooth(method='lm', se=FALSE, col='black', linetype='dashed', linewidth=0.8, inherit.aes=FALSE, aes(x=Date, y=MHHW)) + theme_bw()

2.732001e-05*(365*10)-3.398793e-01

```

This coefficient gives us the average increase in high tide levels each day, across all six cities, for this ten year time frame (i.e. the units of the coefficient are in m per day).

# 8c. Using your estimated coefficient, estimate the mean increase in sea-level over the 10 year time frame from 2011-2020.

```{r}
# (.00002732*365*10) = 
# using LM
predict(model)
p = predict(model, combined[c(1,nrow(combined)), ])
diff(p)
# 0.09895307
# OR
# 2.732001e-05*(365*10) 
# 0.09971804
```


Upload your .Rmd file and you knitted file with the answers and plots to Canvas

##STRETCH GOAL

If you are looking for a challenge, have a go downloading the original csv files directly from the NOAA API. Details on the API are here: <https://api.tidesandcurrents.noaa.gov/api/prod/>

You will want to paste together a URL describing the data you want from the API, then use download.file() to download the data from that URL into a directory on your computer.

The URL you want will have the following form, except you will loop through to replace *GAUGEID* with each of the six tide gauge ID numbers: 

paste0("https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?begin_date=20110101&end_date=20201231&station=",*GAUGEID*,"&product=monthly_mean&datum=MHHW&units=metric&time_zone=lst&format=csv")

See if you can make sense of this URL given the options listed at the website describing access to the API

